\documentclass[12pt]{report}
% had to change latex compiler to XeLatex to get this to work- used to be pdfLatex
\usepackage[utf8]{inputenc}
\usepackage{setspace}
%\usepackage{subfigure}

\pagestyle{plain}
\usepackage{amssymb,graphicx,color}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{latexsym}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tabulary}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{hyperref}

% very basic, number only
%\usepackage{biblatex}
%\addbibresource{refs.bib}


% Goldstone and Rogosky 2002
%\usepackage[backend=biber,
%style=numeric,
%citestyle=authoryear]{biblatex} 
%\addbibresource{refs.bib}

% [GR02]
\usepackage[backend=biber,
style=alphabetic,
citestyle=alphabetic]{biblatex} 
\addbibresource{refs.bib}


\usepackage{todonotes}
\input{defs.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{  	{ \includegraphics[scale=.5]{ucl_logo.png}}\\
{{\Huge Semi-supervised Learning Of Jointly Aligned Concept Embeddings}}\\
%{\large There Is No Optional Subtitle Yet}\\
		}
\date{Submission date: Day Month Year}
\author{Petra Chong\thanks{
{\bf Disclaimer:}
This report is submitted as part requirement for the MSc in Machine Learning at UCL. It is
substantially the result of my own work except where explicitly indicated in the text.
The report may be freely copied and distributed provided the source is explicitly acknowledged.
}
\\ \\
MSc Machine Learning\\ \\
Supervised by: Professor Bradley Love and D.r Brett Roads}

 
\onehalfspacing
\maketitle
\begin{abstract}
We are interested in algorithms that learn like humans. Humans are natural multimodal learners, applying knowledge and skills gained from one task to many other tasks. Studies in multi-task machine learning conclude that this approach of learning from multiple input streams results in computer models that generalise better. We test this by simultaneously learning probabilistic embeddings of concepts for two domains- images and audio- from co-occurrence statistics of concepts in the Open Images and AudioSet datasets. We constrain the problem by inducing alignment during the learning process using the labels present in both datasets as semi-supervised input, resulting in the embeddings for both domains, and the alignment between them, being learned jointly. The intent is that the extra constraint should regularise the problem, reducing the chance of being stuck in local minima as is a known issue with learning of embeddings.  

We compare the cosine similarity of concept pairs from our aligned embeddings with the similarity of pairs from three human-curated datasets containing human similarity judgement (WordNet, enhanced ILSVRC and MTURK-771), as a measure of embedding quality. We find that semi-supervised alignment increases embedding quality of both domains when compared with WordNet, the largest of these human-curated datasets. In particular, the embedding quality of the smaller domain (AudioSet) is increased. 
\end{abstract}

\input{ack.tex}
\tableofcontents
\setcounter{page}{1}

%\input{0_todo.tex}

\input{1_intro.tex}

\input{2_litreview.tex}

\input{3_method.tex}

\input{4_results.tex}

\input{5_conclusions_further.tex}

%\input{xxx_notes}

\appendix

\input{A_appendix.tex}


%\bibliographystyle{apalike}
%\bibliography{refs}

\printbibliography

\end{document}