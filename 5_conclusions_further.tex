\chapter{Further discussion and conclusions}

\todo[inline]{Brief summary of everything so far, suggestions for future work. }

\section{Summary of results}

\subsection{Restatement of project aims}

The main aim was to learn probabilistic embeddings, represented by multidimensional Gaussian distributions with diagonal covariance, for concepts in the Open Images and AudioSet domains. This was done once without alignment, and once with alignment. The quality of these embeddings was examined by comparing the Spearman correlation of their pairwise similarities with three different human-curated similarity measures (only two for AudioSet due to lack of data)- MTURK-771 \cite{mturk771}, WordNet \cite{WordNet}, and ILSVRC \cite{RoadsLoveCVPR}. 

\subsection{Independently learned probabilistic embeddings}

The variances of the independently learned embeddings,  when expressed as entropies,  correlate negatively with co-occurrence frequency. Concepts which occur less frequently in the input data have a higher variance. This is a result which we expect- there is more uncertainty about concepts that occur less frequently. The means of the probabilistic embeddings represent sensible clusters when viewed through t-SNE dimensionality reduction down to 2 dimensions. Qualitatively, items in the concept hierarchy that represent very abstract concepts from the point of view of the hierarchy (``Cat", ``Dog") have nearest neighbours that are not very well clustered (show example). Items that are more specific (``Domestic short-haired cat")  have nearest neighbours that are more sensible (also show example). 

\subsection{Semi-supervised learning of aligned embeddings}

Our algorithm successfully learned jointly aligned embeddings from two modalities, OpenImages containing 19996 concepts and AudioSet containing 526 concepts, with an intersection of 230 concepts. No post-processing of the learned embeddings was necessary for alignment. The 230 intersecting concepts were used as semi-supervised input into the algorithm by tying their mapped values together during training as part of the loss function. 

Alignment accuracy, as measured by the number of concepts in one domain whose nearest neighbour in the mapped domain was the true corresponding concept in the other domain \todo[inline]{Add reference to where this is described} of more than 95\% was obtained between the two domains, for the intersecting concepts. However, convergence However convergence to this level of accuracy while still maintaining sensible clustering (avoiding the degenerate case displayed in figure \ref{fig:dysfunctional_cluster} required the following:

    \begin{itemize}
        \item 10 samples per embedding in the mini-batch needed to be used in each training step. This probably acts like data augmentation. 
        \item GloVe loss to be scaled by the ratio of concepts; since Open Images had 19996 concepts and AudioSet 526, GloVe loss for Open Images was multiplied by 19996 / 526. The cause of this requirement has not been investigated. 
    \end{itemize}
    
The criterion for saving the embeddings was when the mean alignment accuracy (of both OpenImages and AudioSet embeddings) was the lowest. Knowing that the independently learned probabilistic embeddings for AudioSet required more epochs of training to converge than the similar case of Open Images, it is possible that the point of greatest alignment accuracy for AudioSet is at a different epoch than for Open Images, but that training to achieve this would cause a decrease in accuracy for Open Images. 

    
Embedding quality as measured by Spearman correlation of embedding pair similarity and WordNet similarity measures was greater for the aligned Open Images embeddings than the independently learned Open Images embeddings. When measuring by Spearman correlation of embedding pair similarity and ILSVRC / MTURK-771 datasets, embedding quality for the aligned Open Images was decreased compared to the independently learned embeddings.     

The same behaviour was observed with AudioSet embeddings, where embedding quality as judged by the WordNet metric was greater for aligned than independently learned embeddings, and was less for aligned than independent when measured against ILSVC / MTURK-771 datasets. 


Including the empirical MMD statistic in the loss function increased alignment accuracy for both domains, but had varying effects on other measures of embedding quality. The MMD statistic increased embedding quality as measured by the Spearman correlation metric for both Open Images and AudioSet domains, compared to the case of not using the MMD statistic. 

The entropy of the aligned embeddings became decorrelated with the frequency of occurrence of each concept, compared to independently learned embeddings. \todo[inline]{Could we expect that the variance of the distributions is less meaningful? Is there any other analysis of variance we can do?}


\subsection{Computational performance}

When run using one GeForce RTX 2080 GPU, 150 epochs took about half an hour where most of the time taken was actually saving the model to disk when the mean epoch accuracy was greater than the previous epoch's accuracy (the actual model runtime was much less). 

The most significant bottleneck in the process was actually creating the co-occurrence matrix for OpenImages, as there are close to 20000 concepts in the dataset, and this information was contained in a 57 million line text file. However this only needed to be done once. 

\section{Directions for future research}

The sizes of the Open Images and AudioSet datasets are very nbalanced, with 19996 concepts in the former and 526 in the latter. We already came across one issue which was that the GloVe loss needed to be scaled to achieve good convergence from the point of view of alignment accuracy. It is possible that the other losses in the overall loss function need to be tuned to achieve even better convergence. 

The use of alignment accuracy itself as a measure of convergence could be reconsidered for further experiments. We already see that with the use of MMD, alignment accuracy could increase but embedding quality could decrease; perhaps there is a way of including some measure of embedding quality in the stopping criteria or loss function. 

The highly non-overlapping concept sets mean that there are many concepts present in Open Images that are not specifically present in AudioSet. However, the Open Images concepts are actually at many levels of hierarchy. For example there are many different types of cat (Malayan cat, Tabby cat, Siamese cat) and many different types of dog. Thus many of these may actually map to a single concept in AudioSet. 

Incorporating some form of hierarchy may lead to better results. Poincare embeddings \todo[inline]{find citation} use distance measures in non-Euclidean space (hyperbolic in this case) to allow hierarchical concepts to be represented natively. It is clear that there are clusters of concept classes, and that preserving the structure between these clusters is desirable. One obvious modification might be to align the parent concept classes for example ``Cat" and ``Dog" as a first pass, and then to align the subclasses around these anchor concepts. This process of multiple alignment passes has been used in \cite{UnsupervisedAlignmentWP} to perform alignment of cross-lingual word embeddings for bilingual lexicon induction. 

There are many aural concepts that obviously will not have a visual representation, so this is yet another limitation of using these two modalities. Better results may perhaps be obtained by trying to align embeddings derived from Open Images and text, but then we would have to resolve the following technical problems:

\begin{itemize}
    \item How to resolve words to the Open Images namespace; one such possibility was used when running similarity comparisons with WordNet, described in the previous chapter. 
    \item How to extract not just single words but phrases from the text, for example ``domestic short-haired cat"
\end{itemize}


\section{Unsupervised learning of aligned embeddings}

The ultimate goal is to be able to learn aligned embeddings in an unsupervised way, in keeping with the desire to truly mimic human learning. In this situation the concept universes for both domains are known, as are the items in the intersection. However the specific embeddings in the intersection in each domain would not be directly mapped in the losses during learning. Aggregate statistics of the domain intersection might be used, such as the MMD. 

Preliminary work was unable to produce accuracies greater than 1\% for totally unsupervised alignment, using the same model configuration as the semi-supervised case, only excluding the distance loss that related $||f(x) - y||$ and $||g(y) - x||$, even when run to high numbers of epochs. It is possible that including some measure of graph-based similarity between the embeddings may increase convergence. The Friedman-Rafsky statistic described in \cite{torchtwosample} was tried, but there was no perceptible convergence. However, no tuning was done on the model parameters, which may have affected convergence. 

Further preliminary work also tested the Manifold Alignment GAN \cite{magan} and the Wasserstein GAN \cite{WassersteinGAN}, using the configurations described in the respective papers, with the full loss function as in \todo[inline]{insert reference to loss function here} used as the generator loss. Neither of these produced any feasible alignment. Mode collapse (all concepts mapping to the same embedding) was at first an issue, but even after using the minibatch discrimination technique \todo[inline]{CITE}, no feasible alignment was found. The discriminator loss was at minimal values, indicating that the discriminators were not able to tell the mapped values from the real values, but there was still no alignment.  

It is known that GANs do well on problems where the input data fall into specific discernible classes \todo[inline]{find citation}, and our dataset does not have this characteristic (though there are discernible classes, they are indistinct as befits a human taxonomy of concepts rather than visual representations of 10 digits, or works by different artists), it is reasonable that a naive GAN might not work. In particular, the source and target domains are very unbalanced in cardinality, a situation which neither the MAGAN nor Wasserstein GAN were tested with. Additionally, the MAGAN's concept of intersecting shared features meant that some of the dimensions of the data were shared between domains, but not all.