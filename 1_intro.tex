\chapter{Introduction}

To build a system that learns in a similar way to humans, we first have to identify what we want to learn, and the features of human learning that we want to emulate. We then have to find an appropriate computational representation for the learned items, and an algorithm to perform the learning. 

First, we consider how humans represent ideas internally. We take a definition from philosophy \cite{stanfordconcepts}  and cognitive science  \cite{Pinker2007} of concepts as psychological entities that form an internal mental system. In both human and machine representation, the structure formed by the interconnection of these representations is significant. 

%There is often a one-to-one mapping between concepts and words (in human natural language). However, the different human languages often have different levels of expressiveness when it comes to concepts. For example, there are words in some languages that do not have immediate translations in others; the German word ``schadenfreude" does not translate to a single word in English, instead requiring a longer sentence. The noun ``biscuit" has different meanings for American and British people. Therefore we see that concepts are not standardised between people and cultures, and one human's ``concept vocabulary" may not overlap completely with another's. 

In machine learning, we can describe concepts as statistical regularities in the world \cite{RoadsLoveNatureMachineIntelligence}. These are often represented computationally as embeddings- real-valued vectors. Learning these statistical regularities should be done in a way that preserves the concept topology, so that concepts close together semantically should be close together in embedding space. 

Different types of media (for example, text, images and audio) provide different presentations of the same concepts. If human experience can be considered to be the underlying generative process for these different media, we can expect that similar relationships should be found between concepts as expressed in these media. For example, pictures of apples are more likely to contain pears or oranges, than violins and buses, and documents containing the word ``apple" are more likely to have the word ``pear" in close proximity compared to the words ``violin" or ``bus"- so we would like embeddings learned from images and text to display the behaviour that ``apple" is closer to ``pear" than to ``violin" or ``bus". These co-occurrences of which concepts occur together constitute information that can be used to learn concept representations. 

The human learning paradigm that we try to emulate is multi-task learning, where information from multiple streams is used for simultaneous learning. We will do this by learning embeddings to represent human-defined concepts from two sources of media simultaneously. We will use probabilistic embeddings, where each concept is represented as a distribution (i.e., two vectors, one for the mean and one for the variance), as the learned variances will give us information about the uncertainty of the embedding. We impose an alignment constraint during this learning process such that the two resulting embedding structures (one for each domain) should have the same relationships between individual concepts. In this context, alignment means that the correspondence from a concept in one system to its analogue in the other system is known. 

%We have previously considered that the structure of a concept network should be similar regardless of the form of the input data. We now examine how concepts may be learned from multiple modalities in a way that preserves this similarity; when concepts are learned from images or audio, the two resulting structures should have the same relationships between individual concepts. Finding the mapping from a concept in one system to its analogue in another system is known as alignment. 

%Alignment may be done during or after learning of concepts. If done after learning, the methodology involves learning concepts separately from multiple input datasets, and then further learning a transformation from one domain's concepts to the other, or learning transformations from both domains to a single intermediate representation. If alignment is done during the learning process, then the concepts are learned jointly, with no post-processing to achieve alignment. 

%Alignment of concepts has various applications in machine learning, amongst them and not limited to machine translation, object alignment in video, transfer learning, knowledge graph matching and text understanding. The generic problem is one of finding correspondences between regularities over multiple datasets. 


We hope that the resulting aligned multi-modally learned embeddings will be more similar to human representations than uni-modally learned embeddings, in keeping with \cite{OverviewMultiTaskLearning} which finds multi-task machine learning to generalise better. We test this by comparing pairwise similarity of our embeddings (for both the multi- or uni-modally learned cases) with human judgements of pairwise similarity. The ideal outcome is for jointly aligned embeddings to be superior to independently learned embeddings; that is, the data from one domain will induce better quality in the embeddings learned for the other domain. 

The input dataset comprises co-occurrence matrices constructed from the Google Open Images ($19996$ concepts) \cite{openimages} and AudioSet (526 concepts) \cite{audioset} datasets. A full description of the dataset will follow in a later chapter; this dataset contains a total of $20292$ concepts with an overlap of 230 concepts. The concepts present in both domains will be used to perform semi-supervised alignment. 


%\section{Concepts}





%It is reasonable to conclude that if these media represent human-generated embodiments of concepts, the underlying generative process should manifest in the statistics of what concepts occur in which content. 


%Ideally, to mimic human cognitive behaviour, we would learn computational representations such that the representations learned from different domains (say, images and audio) should have similar structures.

%There is evidence \cite{CoocurrenceVisionLanguage2021} that the human brain stores these statistical co-occurrence regularities  as part of its representation of objects, and that response to an object stimulus (visual or text) can be predicted by the category in which that object resides. Hence, this is an appropriate basis for considering machine learning approaches that seek to emulate the mechanisms of human learning. 

%In this project, we use machine learning algorithms to learn numerical representations of individual concepts, with the input data originating from different media types. We will examine how the relationships between these numerical representations correspond to the relationships between human representations of such concepts. 



%By creating computational systems that learn like humans, we may gain insights into the mechanisms of learning (of both humans and computational systems). Humans naturally learn from multiple input streams, and knowledge acquired from learning one particular task is also applied to other tasks. The machine learning equivalent is known as multi-task learning, where more than one objective is minimised at a time, and training signals of one task can affect the signals of other tasks in the same ensemble. A synergistic effect is observed where models learned from multi-task learning tend to generalise better \cite{OverviewMultiTaskLearning}.

%\section{Alignment}


%\section{Project objectives}
%In this project we wish to learn concepts represented as multidimensional vectors known as embeddings. The input data originates from two modalities- images and audio, and a key aim is to jointly learn embeddings that are aligned, so that we know the correspondences between domains, as in the previous definition.

%Both deterministic and probabilistic embeddings are learned, with deterministic embeddings having each concept represented by a single vector, and probabilistic embeddings representing each concept as a distribution (i.e., two vectors, one for the mean and one for the variance). We will examine the learned means and variances, as well as compare the learned aligned embeddings to human similarity metrics. We hope to find that the joint learning of embeddings for both domains, constrained with appropriate losses to induce alignment, will result in embeddings of higher similarity with human judgement.  

%The dataset comprises co-occurrence matrices constructed from the Google Open Images (19996 concepts) \cite{openimages} and AudioSet (526 concepts) \cite{audioset} datasets. A full description of the dataset will follow in a later chapter; this dataset contains a total of 20292 concepts with an overlap of 230 concepts. The concepts present in both domains will be used to perform semi-supervised alignment. We note that this dataset is quite unbalanced; there are almost 40 times more concepts in Open Images than in AudioSet. 

%\subsection{Key points}
%\begin{itemize}
%    \item Learning of probabilistic embeddings and analysis of variance related to %concepts in terms of level of abstraction and hierarchy.
%    \item Techniques for joint learning of aligned embeddings from different modalities %of real datasets, both semi-supervised and unsupervised.
%    \item The role of a distributional distance metric in improving learning of joint alignment.
%    \item The difficulties inherent in alignment of embeddings learned from real co-occurrence data from multiple modalities. 
%\end{itemize}
