\chapter{Results}

\todo[inline]{Analysis of results}

\section{Semi-supervised learning of aligned embeddings}

\subsection{Observations}

\begin{itemize}
    \item Final $x$ and $y$ accuracies are all greater than 95\%.
    \item If only 1 sample of each embedding was taken in each batch, only 90\% accuracy was achieved. This is probably because using 10 samples acts like data augmentation. 
    \item If glove loss was not scaled, the effect of this was to cause the concepts in the intersection to be a totally different distribution from the rest of the distribution.
    \item L2 regularisation of the MLPs did not fix the clustering problem above, the glove loss had to be scaled
\end{itemize}

\subsection{Statistics}

\subsubsection{Mean (Pearson) cross-correlation of self-similarity matrices over different random seeds (1x2, 1x3, ..., 9x10)}
\begin{tabular}{lrrr}
\toprule
Domain &   Independent & Without MMD &  With MMD \\
\midrule
Open Images    & 0.5895 $\pm$ 0.04863 & 0.4738 $\pm$ 0.04391 &     0.4201 $\pm$  0.04008 \\
AudioSet    & 0.4115 $\pm$ 0.04449 &  0.3986 $\pm$ 0.04391  &      0.3455  $\pm$ 0.04250  \\
\bottomrule
\end{tabular}\\

The runs of the independently learned embeddings are much more correlated with each other. The high value very close to 1 means that the similarity matrices (of each run's embeddings with itself) are very similar across runs. This correlation has dropped in the aligned runs, indicating that the output embeddings are less similar to themselves across runs. We can say that the alignment introduces more variation in the embeddings. MMD in particular seems to introduce more variability. 


\subsubsection{Mean Spearman correlation of entropy of a concept with frequency of occurrence of that concept}
\begin{tabular}{lrrr}
\toprule
Domain &   Independent & Without MMD &  With MMD \\
\midrule
Open Images    &  -0.1785 $\pm$ 0.01108 & 0.04208 $\pm$ 0.01864 &     0.03622 $\pm$  0.01152 \\
AudioSet    &  -0.3998 $\pm$ 0.04148 & -0.000539 $\pm$   0.07388 &      0.03590  $\pm$ 0.02592  \\
\bottomrule
\end{tabular}\\

There is a reasonable negative Spearman correlation between the entropy of a concept and its frequency of occurrence, in the independently learned case. Entropies of less frequently occurring concepts are found to be higher. This correlation has largely disappeared in the aligned cases, being roughly around zero. 

\newpage
\subsubsection{Entropy plots}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/method/probabilistic_aligned/openimages/probabilistic_sup_mmd0_150_entropies.png}
    \caption{
        Histogram of entropies for aligned Open Images embeddings, run without MMD. The plot is very faintly bimodal and there is more variability in it over runs with different random seeds than the entropy plot for the independently learned embeddings. 
    }
\end{figure}

\begin{figure}[H]

    \centering
    \includegraphics[width=0.6\textwidth]{images/method/probabilistic_aligned/openimages/probabilistic_sup_mmd100_150_entropies.png}
    \caption{
        Histogram of entropies for aligned Open Images embeddings, run with MMD. There is even more variability in the distributions between runs with different random seeds. 
    }
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/method/probabilistic_aligned/audioset/probabilistic_sup_mmd0_150_entropies.png}
    \caption{
        Histogram of entropies for aligned AudioSet embeddings, run without MMD. The same pattern of bimodality plus greater variability is observed as in Open Images. 
    }
\end{figure}

\begin{figure}[H]

    \centering
    \includegraphics[width=0.6\textwidth]{images/method/probabilistic_aligned/audioset/probabilistic_sup_mmd100_150_entropies.png}
    \caption{
        Histogram of entropies for aligned AudioSet embeddings, run with MMD. Again, the same pattern of even greater variability between runs with different random seeds, is observed as in Open Images. 
    }
\end{figure}

\subsubsection{Alignment accuracy and embedding quality}

\begin{itemize}
    \item Using only distance (supervised) loss had a lower degree of accuracy - 94-95\% 
    \item Adding MMD gave a higher degree of accuracy - 96-97\%
\end{itemize}

High alignment accuracy between embeddings in both domains does not necessarily mean the embeddings are good. The following figure is a t-SNE plot of aligned embeddings (reduced to 2 dimensions) of very high accuracy (97\%), meaning that the embeddings in Open Images and AudioSet are well aligned. However, the actual embeddings did not form good clusters; concepts that were related semantically tended not to be close in embedding space. 

The following figure shows the results of running the alignment algorithm without scaling the GloVe loss:

\begin{figure}[H]
\label{fig:dysfunctional_clusters}
    \centering
    \includegraphics[width=0.95\textwidth]{images/method/probabilistic_aligned/dysfunctional_clusters.png}
    \caption{
        The alignment accuracy for this set of embeddings is greater than 97\%. The blue marks denote concepts that are in the intersection of concepts (present in both Open Images and AudioSet). The orange marks denote concepts that are in the 200 most frequent concepts occurring in Open Images. It is immediately visible that the algorithm has clustered concepts in the intersection degenerately; concepts that are in the intersection are more likely to be close to other concepts in the intersection rather than concepts that are semantically close
    }
\end{figure}

Hence, alignment accuracy is insufficient and we need another metric. To evaluate the quality of the resulting aligned embeddings, we could compare how good they are relative to independently learned embeddings. If our alignment algorithm is good, we should expect that the aligned embeddings are of higher quality than the independently learned embeddings. 

A plausible measure of embedding quality would be to compare the similarity of concept pairs as calculated from embeddings, with the similarity of concept pairs as evaluated by humans. In order to do this, we need to find human-curated datasets.

One such dataset is the MTURK-771 dataset, created by the authors of \cite{mturk771}, a study of learning the relatedness of word pairs. This study also uses the technique of measuring the goodness of their relatedness-learning algorithm by comparing the Spearman correlation of its predictions with the human judgements. The MTURK-771 dataset comprises 771 pairs of words with human-rated similarity scores, collected using the Amazon Mechanical Turk online tool. Unfortunately, the intersection of the 771 word pairs with our Open Images and AudioSet concepts was not large - 171 for Open Images and only 3 for AudioSet. 

The WordNet \cite{WordNet} lexical database contains English nouns, verbs and adjectives grouped according to synonymy (semantic similarity) and hyponymy (hierarchy). Words in WordNet are represented by ``synsets", each of which denotes a particular sense of the word. Therefore, one word may have several synsets within WordNet. The relationships between word senses were encoded from various corpora and thesauri \todo[inline]{this seems wrong but thesauruses sounds stupid???}, which themselves were human-curated, therefore we take the lexical and semantic information contained within WordNet as an appropriate representation of human judgement. The WordNet database can be accessed directly from the \texttt{nltk} Python package. 

Furthermore, there is a large overlap between pairs of words found in WordNet and pairs of concepts from our dataset.  Out of 128005400 nonzero pairs occurring in Open Images, 27052350 are also present in WordNet. Out of 42002 nonzero pairs in AudioSet, 19208 are also present in WordNet. 

A third available dataset originates from a prototype model for \cite{RoadsLoveCVPR}, in which human similarity judgements of pairs of concepts are collected to supplement the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) 2012 task. This dataset will be referred to as the ``enhanced ILSVRC" dataset. These judgements were collected from participants using Amazon Mechanical Turk. The data collection process was carefully curated to assemble a good set of human similarity judgements, using probabilistic techniques for trial selection to maximise the expected information gain from each trial. In this dataset, 147070 pairs overlap with Open Images, and 93 pairs overlap with AudioSet. One word was removed from the AudioSet pairs when computing the Spearman correlation, which is ``tick", because that label as used in AudioSet is used to describe the sound and not the insect. We know that the ImageNet label corresponds to the insect. 

We adopt the same method used by \cite{mturk771} of comparing the Spearman correlation of the similarity of our embedding concept pairs with the similarity of the human-measured dataset to evaluate the quality of aligned embeddings compared to independently learned embeddings. The Spearman correlation is used because we wish to evaluate whether there is a monotonic relationship, and the similarity measures will be on different scales. The cosine similarity measure is used as it contains the dot product, which is an input into the GloVe algorithm that generates the embeddings.

The similarity of two concepts with indexes $i$ and $j$ in embedding space is therefore measured as

\begin{equation}
s_{ij} = 1 - \frac{\vece_i \cdot \vece_j}{||\vece_i||\!||\vece_j||}
\end{equation}

where $\vece_i$ represents the $i$-th embedding .

The MTURK-771 and enhanced ILSVRC datasets are mappings of concept pairs to similarity values. However, there is no such WordNet dataset ready for use, so we have to construct one. WordNet has various available similarity measures between synsets that can be computed directly from the Python library. We choose the Leacock-Chodorow (LCH) similarity measure (https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.1677&rep=rep1&type=pdf), which takes the shortest path between the two synsets, scaled by the maximum depth of the taxonomy \todo[inline]{DEFINE THIS}. Intuitively, if the two synsets are very similar, there will be a short path between them, and they will have a common ancestor that is not many levels up in the hierarchy. 

As a single word or phrase may map to several synsets, and we have no sense information in our Open Images / AudioSet concept names to disambiguate, we have to use a heuristic algorithm for choosing a synset in the cases where there is more than one:

\begin{itemize}
    \item Check every pair in the domain (Open Images or AudioSet) that has nonzero co-occurrence
    \item Convert each word in the pair to lowercase with spaces replaced with underscores, as this is the WordNet naming scheme.
    \item Check if both converted words have synsets represented in WordNet. If either does not have a synset in WordNet, ignore the pair.
    \item For each combination of the first 2 synsets for each word (up to 4 pairs in total), compute the LCH similarity \todo{CITE} if both elements in the pair have the same part of speech. If they do not have the same parts of speech, ignore the pair. \footnote{WordNet similarity is not defined for synsets which do not have the same parts of speech. } This is to handle pairs like ``mandarin orange" and ``orange"; ``mandarin orange" has two synsets, the first of which refers to the mandarin orange tree, and the second of which refers to the fruit. Therefore the second ``mandarin orange" synset matches the first ``orange" synset more closely. 
    \item One mode of failure for this algorithm is for words that occur in both Open Images and AudioSet but with different meanings, for example ``tap" and ``tick" are different words when referring to objects or sounds. 
    \item The WordNet similarity for the pair is taken to be the largest such value. Therefore this algorithm will be biased high; thus if we observe a strong effect in spite of this high bias, it is a stronger result. 
\end{itemize}

If the Spearman correlation between the aligned embedding pair similarity and WordNet pair similarity is higher than the Spearman correlation between the independently learned embedding pair similarity and WordNet pair similarity, it indicates that the aligned embedding pairs are in aggregate more like human judgement of similarity than the independent embedding pairs. Since our goal is to obtain embeddings that are more similar to human judgement, this would translate to the aligned embeddings being of higher quality, by this metric. We can generalise this to other model variants, for example aligning with or without the MMD loss, to evaluate the effect of those model variants. 

\subsubsection{Embedding stability}

Another metric of embedding quality is the stability over multiple runs. As previously mentioned, the stochasticity in the GloVe embedding training means that different embeddings are produced for different runs, corresponding to different local minima. The structures of these are not the same from run to run, as we saw in a table of the top 5 nearest neighbours for various concepts, over different random seeds \todo[inline]{reference to table of 5NN for Cat / Domestic short-haird cat / etc}. We try to quantify stability as follows:

In addition to the metric mentioned in \cite{WordEmbeddingStability}, a simple count of the number of intersecting concepts in the 5 nearest neighbours of any concept, over 10 seeds (divided by 5 to normalise to 1, we define an alternate similarity metric which is easier to plot and visualise the distribution. 

This is as follows: 

\begin{itemize}
    \item Measure the size of the union of all the top 5 nearest neighbours to a concept, by Euclidean distance, for 10 random seeds. Let this be $C$. 
    \item Take the mean of $5/C$ for the top 300 concepts in Open Images and Audioset.
    \item If the embeddings were very stable, then the top 5 nearest neighbours to a concept over all runs should be the same, and we will have a number close to 1. The smaller it is, the less stable the embeddings are. 
\end{itemize}

\subsubsection{Mean stability of embeddings by model variant}

Empirically, alignment appears to decrease stability of the embeddings, meaning that the nearest neighbours of a particular concept (measured by Euclidean distance) tend to change more over different random seeds. Intuitively this is reasonable. Training aligned probabilistic embeddings involves more variables and a more complex loss function, so we might expect the variances to be greater. 

% generated by inspect_results.py: print(sdf.to_latex())
\begin{tabular}{lllrr}
\toprule
Model name  & MMD    & Domain          & Union metric & Intersection metric         \\
\midrule
AlignedGlove & 0   & openimages &  0.237304 & 0.07333\\
            &     & audioset &  0.210765 & 0.053512\\
            & 100 & openimages &  0.210257 & 0.052667\\
                   &     & audioset &  0.224780 & 0.081605\\
ProbabilisticGlove & 0   & openimages &  0.289989 & 0.101333 \\
                   &     & audioset &  0.39005 & 0.331104 \\
\bottomrule
\end{tabular}\\

\subsubsection{Plots of stability by relative frequency}

The per-item plots show very clearly the distribution of the stability values per rank of concept frequency. For the independently learned embeddings, some concepts have a stability of 1.0 which means that over 10 random seeds, the 5 nearest neighbours of those concepts did not change over the run. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{images/results/audioset_stability.png}
    \includegraphics[width=0.45\textwidth]{images/results/audioset_stability_ixn.png}
    \caption{
        Stability of model variants for AudioSet. y-axis is relative frequency of concepts (most frequent = 0)
    }
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{images/results/openimages_stability.png}
    \includegraphics[width=0.45\textwidth]{images/results/openimages_stability_ixn.png}
    \caption{
        Stability of model variants for Open Images. y-axis is relative frequency of concepts (most frequent = 0)
    }
\end{figure}

\subsection{Comparison with MTURK-771}

\todo[inline]{Condense all tables into one}

Only Open Images was compared with this dataset, as there were too few pairs overlapping with AudioSet (only three). 

\subsubsection{Open Images, Spearman correlation with MTURK-771 similarity}\\

\begin{tabular}{lrrrrr}
\toprule
{} &  independent &   aligned &  aligned\_acc &  aligned\_mmd &  aligned\_mmd\_acc \\
\midrule
1    &     0.357040 &  0.305814 &     0.947826 &     0.331085 &         0.947826 \\
2    &     0.342876 &  0.271306 &     0.956522 &     0.296354 &         0.982609 \\
3    &     0.375564 &  0.338173 &     0.934783 &     0.287192 &         0.956522 \\
4    &     0.342911 &  0.299614 &     0.952174 &     0.289880 &         0.965217 \\
5    &     0.357850 &  0.255958 &     0.947826 &     0.225208 &         0.960870 \\
6    &     0.373989 &  0.308962 &     0.952174 &     0.271217 &         0.973913 \\
7    &     0.347433 &  0.296559 &     0.965217 &     0.292202 &         0.965217 \\
8    &     0.343246 &  0.284002 &     0.969565 &     0.283377 &         0.952174 \\
9    &     0.340476 &  0.279059 &     0.956522 &     0.271877 &         0.956522 \\
10   &     0.366640 &  0.267160 &     0.960870 &     0.283802 &         0.952174 \\
\midrule
mean &     0.354803 &  0.290661 &     0.954348 &     0.283219 &         0.961304 \\
\bottomrule
\end{tabular}\\


\subsubsection{Open Images, differences of Spearman correlation between aligned model variant and MTURK-771 and independent embeddings}

\begin{tabular}{lrr}
\toprule
{} &   aligned &  aligned\_mmd \\
\midrule
1    & -0.051227 &    -0.025955 \\
2    & -0.071571 &    -0.046522 \\
3    & -0.037390 &    -0.088372 \\
4    & -0.043297 &    -0.053031 \\
5    & -0.101893 &    -0.132642 \\
6    & -0.065027 &    -0.102772 \\
7    & -0.050874 &    -0.055231 \\
8    & -0.059244 &    -0.059869 \\
9    & -0.061417 &    -0.068599 \\
10   & -0.099481 &    -0.082839 \\
\midrule
mean & -0.064142 &    -0.071583 \\
\bottomrule
\end{tabular}\\



\subsection{Comparison with WordNet}

\subsubsection{Open Images, Spearman correlation with WordNet similarity}

\begin{tabular}{lrrrrr}
\toprule
{} &  independent &   aligned &  aligned\_acc &  aligned\_mmd &  aligned\_mmd\_acc \\
\midrule
1    &     0.204645 &  0.229008 &     0.947826 &     0.228036 &         0.947826 \\
2    &     0.195614 &  0.221148 &     0.956522 &     0.230173 &         0.982609 \\
3    &     0.199725 &  0.239952 &     0.934783 &     0.239973 &         0.956522 \\
4    &     0.210023 &  0.233182 &     0.952174 &     0.228500 &         0.965217 \\
5    &     0.190570 &  0.225954 &     0.947826 &     0.226141 &         0.960870 \\
6    &     0.206630 &  0.238630 &     0.952174 &     0.236737 &         0.973913 \\
7    &     0.207851 &  0.229461 &     0.965217 &     0.238660 &         0.965217 \\
8    &     0.215203 &  0.217090 &     0.969565 &     0.218335 &         0.952174 \\
9    &     0.216922 &  0.223884 &     0.956522 &     0.233538 &         0.956522 \\
10   &     0.206950 &  0.225371 &     0.960870 &     0.230410 &         0.952174 \\
\midrule
mean &     0.205413 &  0.228368 &     0.954348 &     0.231050 &         0.961304 \\
\bottomrule
\end{tabular}

\subsubsection{Open Images, differences of Spearman correlation between aligned model variant and WordNet and independent embeddings} \\

\begin{tabular}{lrr}
\toprule
{} &   aligned &  aligned\_mmd \\
\midrule
1    &  0.024363 &     0.023391 \\
2    &  0.025535 &     0.034559 \\
3    &  0.040227 &     0.040248 \\
4    &  0.023160 &     0.018477 \\
5    &  0.035384 &     0.035571 \\
6    &  0.032000 &     0.030108 \\
7    &  0.021610 &     0.030809 \\
8    &  0.001887 &     0.003131 \\
9    &  0.006962 &     0.016616 \\
10   &  0.018421 &     0.023459 \\
\midrule
mean &  0.022955 &     0.025637 \\
\bottomrule
\end{tabular}


\subsubsection{AudioSet, Spearman correlation with WordNet similarity}

\begin{tabular}{lrrrrr}
\toprule
{} &  independent &   aligned &  aligned\_acc &  aligned\_mmd &  aligned\_mmd\_acc \\
\midrule
1    &     0.138980 &  0.162728 &     0.947826 &     0.152753 &         0.965217 \\
2    &     0.132803 &  0.130027 &     0.969565 &     0.147193 &         0.982609 \\
3    &     0.157127 &  0.174757 &     0.956522 &     0.133395 &         0.978261 \\
4    &     0.146678 &  0.148297 &     0.956522 &     0.168750 &         0.956522 \\
5    &     0.116176 &  0.151164 &     0.943478 &     0.152322 &         0.969565 \\
6    &     0.144309 &  0.134435 &     0.973913 &     0.125409 &         0.969565 \\
7    &     0.149543 &  0.133959 &     0.947826 &     0.169722 &         0.973913 \\
8    &     0.140360 &  0.182683 &     0.960870 &     0.127695 &         0.952174 \\
9    &     0.147353 &  0.135669 &     0.960870 &     0.187408 &         0.978261 \\
10   &     0.149795 &  0.165782 &     0.965217 &     0.142835 &         0.978261 \\
\midrule
mean &     0.142313 &  0.151950 &     0.958261 &     0.150748 &         0.970435 \\
\bottomrule
\end{tabular}


\subsubsection{AudioSet, differences of Spearman correlation between aligned model variant and WordNet and independent embeddings with WordNet}

\begin{tabular}{lrr}
\toprule
{} &   aligned &  aligned\_mmd \\
\midrule
1    &  0.023748 &     0.013773 \\
2    & -0.002776 &     0.014390 \\
3    &  0.017631 &    -0.023732 \\
4    &  0.001618 &     0.022072 \\
5    &  0.034988 &     0.036146 \\
6    & -0.009875 &    -0.018901 \\
7    & -0.015584 &     0.020179 \\
8    &  0.042323 &    -0.012665 \\
9    & -0.011684 &     0.040055 \\
10   &  0.015986 &    -0.006961 \\
\midrule
mean &  0.009637 &     0.008436 \\
\bottomrule
\end{tabular}

\subsection{Comparison with ILSVRC}

\subsubsection{Open Images, Spearman correlation with ILSVRC similarity}

\begin{tabular}{lrrrrr}
\toprule
{} &  independent &   aligned &  aligned\_acc &  aligned\_mmd &  aligned\_mmd\_acc \\
\midrule
1    &     0.523743 &  0.493222 &     0.947826 &     0.488418 &         0.947826 \\
2    &     0.470860 &  0.460144 &     0.956522 &     0.457864 &         0.982609 \\
3    &     0.482953 &  0.486438 &     0.934783 &     0.433406 &         0.956522 \\
4    &     0.514398 &  0.531787 &     0.952174 &     0.482189 &         0.965217 \\
5    &     0.504501 &  0.465994 &     0.947826 &     0.456137 &         0.960870 \\
6    &     0.541152 &  0.472602 &     0.952174 &     0.450753 &         0.973913 \\
7    &     0.506350 &  0.468013 &     0.965217 &     0.446900 &         0.965217 \\
8    &     0.553240 &  0.458478 &     0.969565 &     0.468173 &         0.952174 \\
9    &     0.507937 &  0.463128 &     0.956522 &     0.438866 &         0.956522 \\
10   &     0.531030 &  0.463582 &     0.960870 &     0.451747 &         0.952174 \\
\midrule
mean &     0.513617 &  0.476339 &     0.954348 &     0.457445 &         0.961304 \\
\bottomrule
\end{tabular}

\subsubsection{Open Images, differences of Spearman correlation between aligned model variant and ILSVRC and independent embeddings} \\

\begin{tabular}{lrr}
\toprule
{} &   aligned &  aligned\_mmd \\
\midrule
1    & -0.030521 &    -0.035326 \\
2    & -0.010716 &    -0.012996 \\
3    &  0.003485 &    -0.049548 \\
4    &  0.017389 &    -0.032209 \\
5    & -0.038507 &    -0.048364 \\
6    & -0.068550 &    -0.090399 \\
7    & -0.038337 &    -0.059450 \\
8    & -0.094763 &    -0.085067 \\
9    & -0.044809 &    -0.069071 \\
10   & -0.067448 &    -0.079283 \\
\midrule
mean & -0.037278 &    -0.056171 \\
\bottomrule
\end{tabular}

\subsubsection{AudioSet, Spearman correlation with ILSVRC similarity}

\begin{tabular}{lrrrrr}
\toprule
{} &  independent &   aligned &  aligned\_acc &  aligned\_mmd &  aligned\_mmd\_acc \\
\midrule
1    &     0.634553 &  0.683402 &     0.947826 &     0.502208 &         0.965217 \\
2    &     0.584539 &  0.686640 &     0.969565 &     0.650786 &         0.982609 \\
3    &     0.646683 &  0.427680 &     0.956522 &     0.617364 &         0.978261 \\
4    &     0.560294 &  0.609964 &     0.956522 &     0.648891 &         0.956522 \\
5    &     0.562203 &  0.513384 &     0.943478 &     0.578213 &         0.969565 \\
6    &     0.630897 &  0.609099 &     0.973913 &     0.503820 &         0.969565 \\
7    &     0.539494 &  0.563158 &     0.947826 &     0.546268 &         0.973913 \\
8    &     0.552983 &  0.674972 &     0.960870 &     0.681851 &         0.952174 \\
9    &     0.523634 &  0.718003 &     0.960870 &     0.680956 &         0.978261 \\
10   &     0.607010 &  0.717317 &     0.965217 &     0.698815 &         0.978261 \\
\midrule
mean &     0.584229 &  0.620362 &     0.958261 &     0.610917 &         0.970435 \\
\bottomrule
\end{tabular}


\subsubsection{AudioSet, differences of Spearman correlation between aligned model variant and ILSVRC and independent embeddings with ILSVRC}


\begin{tabular}{lrr}
\toprule
{} &   aligned &  aligned\_mmd \\
\midrule
1    &  0.048850 &    -0.132345 \\
2    &  0.102101 &     0.066247 \\
3    & -0.219003 &    -0.029319 \\
4    &  0.049670 &     0.088598 \\
5    & -0.048820 &     0.016010 \\
6    & -0.021799 &    -0.127078 \\
7    &  0.023664 &     0.006774 \\
8    &  0.121990 &     0.128868 \\
9    &  0.194369 &     0.157321 \\
10   &  0.110307 &     0.091806 \\
\midrule
mean &  0.036133 &     0.026688 \\
\bottomrule
\end{tabular}


\subsection{Discussion}

\subsubsection{Open Images}

For Open Images, using the WordNet comparison metric, it appears that the aligned embeddings are more correlated with human similarity than the independently learned embeddings. The reverse is observed with the MTURK-771 comparison metric, but we do note that this is a very small dataset of only 170 pairs present in both Open Images (out of 120 million pairs) and MTURK-771. The same phenomenon as with MTURK-771 is also observed when comparing with the ILSVRC metric, where the aligned embeddings are less correlated with human judgement than the independently learned embeddings degree of correlation with human judgement. However, the ILSVRC dataset is oddly unbalanced. There are 1000 concepts present in it, of which 124 are different breeds of dog. In fact, 398 of the concepts present in the ILSVRC are different types of animal. There are some concepts in the selected 1000 that almost certainly are not amongst the most common humanly known concepts, for example, ``shoji" and ``bicycle-built-for-two" (but ``bicycle" is not present). The only flowers present are ``daisy" and ``yellow lady's slipper". In short, the ILSVRC dataset does not appear to be a very good sample of human concepts. To investigate whether the unbalancedness of the dataset was affecting results, runs were tried of the ILSVRC dataset excluding all the animals and then only including animals, but the results were broadly the same. 

Using the MMD as a component of the loss appears to increase accuracy, as well alignment quality as measured against the WordNet metric (as the aligned models' cosine similarity has higher Spearman correlation with the WordNet similarity metric). 

\subsubsection{AudioSet}

AudioSet was compared only with WordNet and ILSVRC, as with only three pairs intersecting with MTURK-771, it was impossible to get any meaningful results. 

When compared with both WordNet and ILSVRC, AudioSet aligned embeddings are more correlated with uman similarity judgement than the independently learned embeddings. However, the overlap of pairs with ILSVRC is only 93, so it is not a large sample for comparison. 

Including MMD increases accuracy, but it slightly decreases alignment quality when compared to independently learned embeddings. 

\subsubsection{The role of MMD}

Using MMD as a loss function has the effect of forcing $f(x)$ and $y$ to have the same distribution, and  $g(y)$ and $x$ to have the same distribution. Therefore it is reasonable that it should have a positive effect on alignment accuracy. However if MMD is high relative to glove loss, there is more pressure for alignment but less pressure for good embeddings which can result in dysfunctional embeddings where all the concepts in the intersection are clustered together as shown in figure \ref{fig:dysfunctional_clusters}

\subsubsection{Entropies and variances}

The entropies no longer correlate negatively with frequency of concepts, in the aligned embeddings. In fact from examining the correlation, it is more appropriate to say they are now close to independent of the frequency of concepts, since the correlation is near zero. 

\todo[inline]{Add more analysis}